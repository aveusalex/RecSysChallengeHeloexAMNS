{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab86cf39",
   "metadata": {},
   "source": [
    "Utilizando VGG16 para gerar embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5f8a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Carregar o modelo pré-treinado\n",
    "model = torch.hub.load('pytorch/vision:v0.9.0', 'vgg16', pretrained=True)\n",
    "model = model.to('cuda')\n",
    "model.eval()\n",
    "\n",
    "#Transformações para as imagens\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "#Diretório das imagens\n",
    "image_dir = r\"C:\\Users\\heloi\\Downloads\\yelp_photos\\photos\"\n",
    "\n",
    "#Loop para extrair embeddings de todas as imagens do diretório\n",
    "embeddings = []\n",
    "for filename in tqdm(os.listdir(image_dir)):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        try:\n",
    "            filepath = os.path.join(image_dir, filename)\n",
    "\n",
    "            # Carregar e transformar a imagem\n",
    "            image = Image.open(filepath).convert(\"RGB\")\n",
    "            image_tensor = transform(image).unsqueeze(0)\n",
    "            image_tensor = image_tensor.to('cuda')\n",
    "            \n",
    "            # Obter o embedding da imagem\n",
    "            with torch.no_grad():\n",
    "                embedding = model(image_tensor).squeeze().cpu().numpy()\n",
    "                embeddings.append(embedding)\n",
    "        except:\n",
    "            print(f\"Imagem {filename} não pode ser lida, foi descartada.\")\n",
    "#Salvar embeddings em um arquivo npy\n",
    "embeddings = np.array(embeddings)\n",
    "np.save(\"embeddingsimagens.npy\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb806a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199992, 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Carrega o arquivo de embeddings\n",
    "embeddings = np.load('embeddingsimagens.npy')\n",
    "\n",
    "# Verifica as dimensões dos embeddings\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9533923",
   "metadata": {},
   "source": [
    "É possível verificar que de todas as 200098 imagens, geramos apenas 199992, visto que algumas imagens estavam corrompidas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
